{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: BeautifulSoup4 in /Users/shaquillejohnson/.virtualenvs/cyber/lib/python2.7/site-packages\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import pip\n",
    "def install(package):\n",
    "   pip.main(['install', package])\n",
    "\n",
    "install('BeautifulSoup4')\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.corpus import stopwords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_clean(file_name, num_reviews):\n",
    "    count = 0\n",
    "    output = []\n",
    "    output_text = [] \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    with open(file_name) as f:\n",
    "        while count < num_reviews:\n",
    "            line = f.next()\n",
    "            line = ast.literal_eval(line)\n",
    "            line = json.dumps(line)\n",
    "            line = json.loads(line)\n",
    "            text = line[\"text\"].encode('utf-8').strip().replace(\"\\n\",\"\")\n",
    "            text = re.sub(r\"http\\S+\", \"\", line['text'])\n",
    "            text = re.sub(r\"@\\S+\", \"\", text)\n",
    "            text = re.sub(r\"#\\S+\", \"\", text)\n",
    "            text = text.split()\n",
    "            text = [w for w in text if not text in stopwords.words(\"english\")]\n",
    "            text = ' '.join(text)\n",
    "            output.append((line[\"id\"], text))\n",
    "            output_text.append(text)\n",
    "            count += 1\n",
    "    return output, output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_paris, output_text_paris = data_clean(\"../data/paris_shooting.txt\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_airasia, output_text_airasia = data_clean(\"../data/airasia_crash.txt\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_irene, output_text_irene = data_clean(\"../data/irene_hurricane.txt\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107146911668109313, u'hurricane irene can suck it so hard.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_irene[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_japan, output_text_japan = data_clean(\"../data/japan_tsunami.txt\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1536472088,\n",
       " u'RT Massive earthquake hits Japan. Here& how you can help victims of the earthquake and tsunami in Japan. ...')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_japan[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_london, output_text_london = data_clean(\"../data/london_riot.txt\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101044899578785793,\n",
       " u'Top story: get your tank refill now BBC News - Live: UK riots see more')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_london[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loop(output_text, list_input):\n",
    "    for i in range(len(list_input)):\n",
    "        output_text.append(list_input[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_text = []\n",
    "loop(output_text, output_text_paris)\n",
    "loop(output_text, output_text_airasia)\n",
    "loop(output_text, output_text_irene)\n",
    "loop(output_text, output_text_japan)\n",
    "loop(output_text, output_text_london)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('training_set.txt','w',)\n",
    "for i in output_text:\n",
    "    f.write(i.encode('utf-8'))\n",
    "    f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None,max_features = 500)\n",
    "train_data_features = vectorizer.fit_transform(output_text)\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 500)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "train_data_features_tfidf = tfidf.fit_transform(train_data_features)\n",
    "train_data_features_tfidf = train_data_features_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "train_data_features_tfidf=pca.fit_transform(train_data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 10)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5,n_jobs=-1).fit(train_data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(train_data_features_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_to_centroid_1 = []\n",
    "for i in range(len(train_data_features_tfidf)):\n",
    "    distance_to_centroid_1.append(np.linalg.norm(train_data_features_tfidf[i]-kmeans.cluster_centers_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_to_centroid_2 = []\n",
    "for i in range(len(train_data_features_tfidf)):\n",
    "    distance_to_centroid_2.append(np.linalg.norm(train_data_features_tfidf[i]-kmeans.cluster_centers_[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_to_centroid_3 = []\n",
    "for i in range(len(train_data_features_tfidf)):\n",
    "    distance_to_centroid_3.append(np.linalg.norm(train_data_features_tfidf[i]-kmeans.cluster_centers_[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_to_centroid_4 = []\n",
    "for i in range(len(train_data_features_tfidf)):\n",
    "    distance_to_centroid_4.append(np.linalg.norm(train_data_features_tfidf[i]-kmeans.cluster_centers_[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance_to_centroid_5 = []\n",
    "for i in range(len(train_data_features_tfidf)):\n",
    "    distance_to_centroid_5.append(np.linalg.norm(train_data_features_tfidf[i]-kmeans.cluster_centers_[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3212"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_centroid_1.index(min(distance_to_centroid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Whoa, Just Seen Some Footy From The Japan Shit. Damn, Looks Like Some Movie Shit. i Can& Even Think Of A joke Right Now.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[3212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1259"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_centroid_2.index(min(distance_to_centroid_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"Tangeraanngg i'm comiinnggg!! \\U0001f601\\U0001f633 btw thanks\\u2026 (w/ Akbar at Juanda International Airport Surabaya Terminal 2) \\u2014\""
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[1259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1101"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_centroid_3.index(min(distance_to_centroid_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'RT \"AirAsia Flight 8501 has lost contact with air traffic control in Jakarta, Indonesia, the airline says.\" via'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[1101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_centroid_4.index(min(distance_to_centroid_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'France hunts for gunmen who killed 12 people at Paris magazine'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[641]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1805"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance_to_centroid_5.index(min(distance_to_centroid_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'RT Air Asia flight QZ 8501 to Singapore loses contact, 155 passengers on board'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text[1805]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted = kmeans.predict(train_data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 596, 1: 0, 2: 0, 3: 404, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "summary_paris = {}\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "for i in range(1000):\n",
    "    if predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "    elif predicted[i] == 2:\n",
    "        count_2 += 1\n",
    "    elif predicted[i] == 3:\n",
    "        count_3 += 1\n",
    "    else:\n",
    "        count_4 += 1\n",
    "summary_paris[0] = count_0\n",
    "summary_paris[1] = count_1\n",
    "summary_paris[2] = count_2\n",
    "summary_paris[3] = count_3\n",
    "summary_paris[4] = count_4\n",
    "print summary_paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 307, 1: 103, 2: 236, 3: 0, 4: 354}\n"
     ]
    }
   ],
   "source": [
    "summary_AirAsia = {}\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "for i in range(1000,2000):\n",
    "    if predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "    elif predicted[i] == 2:\n",
    "        count_2 += 1\n",
    "    elif predicted[i] == 3:\n",
    "        count_3 += 1\n",
    "    else:\n",
    "        count_4 += 1\n",
    "summary_AirAsia[0] = count_0\n",
    "summary_AirAsia[1] = count_1\n",
    "summary_AirAsia[2] = count_2\n",
    "summary_AirAsia[3] = count_3\n",
    "summary_AirAsia[4] = count_4\n",
    "print summary_AirAsia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1000, 1: 0, 2: 0, 3: 0, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "summary_Japan = {}\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "for i in range(2000,3000):\n",
    "    if predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "    elif predicted[i] == 2:\n",
    "        count_2 += 1\n",
    "    elif predicted[i] == 3:\n",
    "        count_3 += 1\n",
    "    else:\n",
    "        count_4 += 1\n",
    "summary_Japan[0] = count_0\n",
    "summary_Japan[1] = count_1\n",
    "summary_Japan[2] = count_2\n",
    "summary_Japan[3] = count_3\n",
    "summary_Japan[4] = count_4\n",
    "print summary_Japan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1000, 1: 0, 2: 0, 3: 0, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "summary_London = {}\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "for i in range(3000,4000):\n",
    "    if predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "    elif predicted[i] == 2:\n",
    "        count_2 += 1\n",
    "    elif predicted[i] == 3:\n",
    "        count_3 += 1\n",
    "    else:\n",
    "        count_4 += 1\n",
    "summary_London[0] = count_0\n",
    "summary_London[1] = count_1\n",
    "summary_London[2] = count_2\n",
    "summary_London[3] = count_3\n",
    "summary_London[4] = count_4\n",
    "print summary_London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1000, 1: 0, 2: 0, 3: 0, 4: 0}\n"
     ]
    }
   ],
   "source": [
    "summary_Irene = {}\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "count_2 = 0\n",
    "count_3 = 0\n",
    "count_4 = 0\n",
    "for i in range(3000,4000):\n",
    "    if predicted[i] == 0:\n",
    "        count_0 += 1\n",
    "    elif predicted[i] == 1:\n",
    "        count_1 += 1\n",
    "    elif predicted[i] == 2:\n",
    "        count_2 += 1\n",
    "    elif predicted[i] == 3:\n",
    "        count_3 += 1\n",
    "    else:\n",
    "        count_4 += 1\n",
    "summary_Irene[0] = count_0\n",
    "summary_Irene[1] = count_1\n",
    "summary_Irene[2] = count_2\n",
    "summary_Irene[3] = count_3\n",
    "summary_Irene[4] = count_4\n",
    "print summary_Irene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.91261295,  1.26007273,  1.24407395,  0.62881109,  1.03383773],\n",
       "       [ 0.91261295,  1.26007273,  1.24407395,  0.62881109,  1.03383773],\n",
       "       [ 0.440411  ,  0.97830052,  0.95916945,  0.34366354,  0.65933913],\n",
       "       ..., \n",
       "       [ 0.23236274,  0.90270942,  0.8984772 ,  0.57397626,  0.55234505],\n",
       "       [ 0.28457291,  0.92099734,  0.89860009,  0.62333369,  0.64951875],\n",
       "       [ 0.17321711,  0.88147161,  0.88334459,  0.57708883,  0.58967053]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.transform(train_data_features_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.12883303,  0.68326897, -0.19917747, -0.17195979,  0.04804421,\n",
       "        0.31996497, -0.02112298, -0.20635099,  0.09328551,  0.26723798])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('labels.txt', 'w')\n",
    "for i in range(len(labels)):\n",
    "    f.write(labels[i])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    print(labels[i])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
